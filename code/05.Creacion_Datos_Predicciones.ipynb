{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANEXO 05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de series temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de este codigo es modelar los datos procesados en el notebook \"data_preparation_anexo.ipynb\". Además de los datos procesados, también se recoge el filtro creado en el mismo notebook para poder aplicar el modelo a todos los diferentes escenarios requeridos por naturgy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# Modelo de series temporales que vamos a utilizar.\n",
    "from prophet import Prophet\n",
    "#---------------------------------------------------------\n",
    "\n",
    "# Eliminamos los warnings del notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del dataset y del filtro generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df_temporal_series\n",
    "df = df_temporal_series.copy() \n",
    "\n",
    "%store -r nfilasXcombinacion_selected\n",
    "nfilasXcombinacion_selected = nfilasXcombinacion_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecucion del modelo para obtener predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente trozo de codigo está dividido en dos partes. Esto es debido a las limitaciones del kernel de jupyter notebook para procesar todos los datos a la vez. Al menos en la manera en la que lo hemos intentado nosotros.\n",
    "\n",
    "El bucle aplicará el modelo sobre cada una de las columnas indicadas. Recogerá las predicciones de cada escenarios posible aplicando el filtro anteriormente mencionado, y las unirá a un dataframe temporal. Se calculan aproximadamente 1000 escenarios diferentes. \n",
    "\n",
    "Este dataframe temporal a su vez se añadirá a un dataframe definitivo en el que se irán añadiendo todos los datos reales y predichos de todos los escenarios con el formato necesario para poder representarse en power bi.\n",
    "\n",
    "Este codigo tarda en ejercutarse aproximadamente 1h y 30 min y devolverá seis archivos \".csv\" para cada una de las columnas predichas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_predictions = pd.DataFrame()\n",
    "\n",
    "columns = ['Altas M', 'Altas_Mes_C+T', 'Altas_Mes_Tra', 'Bajas M', 'Bajas_Mes_C+T', 'Bajas_Mes_Tra']\n",
    "\n",
    "for column in columns:\n",
    "\n",
    "    df_predictions = pd.DataFrame()\n",
    "\n",
    "    for combinacion in nfilasXcombinacion_selected:\n",
    "\n",
    "        Var_split = str(combinacion).split('_')\n",
    "            \n",
    "        ID_SOCIEDAD = Var_split[0]\n",
    "        ID_SEGMENTO = Var_split[1]\n",
    "        SEGMENTO2 = Var_split[2]\n",
    "        ID_LINEA_PRODUCTO_AGREG = float(Var_split[3].split(':')[1])\n",
    "        ID_AGRUP_PRODUCTO = float(Var_split[4].split(':')[1])\n",
    "        ID_AGRUP_PROD = Var_split[5]\n",
    "        ID_AGRUP_CANAL = float(Var_split[6].split(':')[1])\n",
    "        ID_ZONA = Var_split[7]\n",
    "\n",
    "        temp_df = df[(df.ID_SOCIEDAD == ID_SOCIEDAD) & \n",
    "                                            (df.ID_SEGMENTO == ID_SEGMENTO) & \n",
    "                                            (df.SEGMENTO2 == SEGMENTO2) &\n",
    "                                            (df.ID_LINEA_PRODUCTO_AGREG == ID_LINEA_PRODUCTO_AGREG) & \n",
    "                                            (df.ID_AGRUP_PRODUCTO == ID_AGRUP_PRODUCTO) &\n",
    "                                            (df.ID_AGRUP_PROD == ID_AGRUP_PROD) & \n",
    "                                            (df.ID_AGRUP_CANAL == ID_AGRUP_CANAL) & \n",
    "                                            (df.ID_ZONA == ID_ZONA)]\n",
    "\n",
    "        # Seleccionando las columnas necesarias para aplicar prophet\n",
    "        temp_df_set = temp_df[['fecha_semanas', column]]\n",
    "\n",
    "        # Renombrando columnas para poder aplicar prophet\n",
    "        temp_df_set.rename(columns = {'fecha_semanas' : 'ds', column : 'y'}, inplace=True)\n",
    "\n",
    "        # Creando un modelo temporal prophet\n",
    "        temp_model = Prophet()\n",
    "\n",
    "        # Ajustando el modelo de prophet\n",
    "        temp_model.fit(temp_df_set)\n",
    "\n",
    "        # Generando el objeto para poder hacer predicciones\n",
    "        temp_future = temp_model.make_future_dataframe(periods = 133, freq = 'W')\n",
    "\n",
    "        # Haciendo predicciones\n",
    "        temp_forecast = temp_model.predict(temp_future)\n",
    "\n",
    "        # renaming columns to get the final dataframe\n",
    "        temp_forecast.rename(columns = {'ds' : 'fecha_semanas'}, inplace=True)\n",
    "\n",
    "\n",
    "        # Dataframe temporal con los datos predichos de un escenario\n",
    "        temp_df_predictions = pd.merge(temp_df, \n",
    "                            temp_forecast[['fecha_semanas', 'yhat', 'yhat_lower', 'yhat_upper']],\n",
    "                            how= 'right',\n",
    "                            on = 'fecha_semanas')\n",
    "\n",
    "        # Este  codigo recoge solo las variables necesarias para BI desde el dataset original. \n",
    "        # Genera el dataset final con todos los escenarios posibles para cada columna\n",
    "        # Y añade datos de los distintos escenarios para las predicciones para poder visualizarlo en BI\n",
    "        df_predictions = df_predictions.append(temp_df_predictions.sort_values('fecha_semanas')[['ID_SOCIEDAD', 'ID_SEGMENTO', 'SEGMENTO2', 'ID_LINEA_PRODUCTO_AGREG', 'ID_AGRUP_PRODUCTO', 'ID_AGRUP_PROD',\n",
    "                                                'ID_AGRUP_CANAL', 'ID_ZONA']].fillna(method='ffill').join(temp_df_predictions.sort_values('fecha_semanas')['Cartera_Acu']).join(temp_df_predictions.sort_values('fecha_semanas')[[column,\n",
    "                                                'fecha_semanas', 'yhat', 'yhat_lower', 'yhat_upper']]))\n",
    "    \n",
    "\n",
    "    # Renombramos columnas segun la columna predicha\n",
    "    df_predictions.rename(columns = {'yhat' : 'yhat_' + column, 'yhat_lower' : 'yhat_lower_' + column, 'yhat_upper': 'yhat_upper_' + column}, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Renombramos las columnas para que sean compatibles con Power BI\n",
    "    if column == 'Bajas_Mes_C+T':\n",
    "            df_predictions.rename(columns = {column: 'Bajas_Mes_CT', 'yhat_' + column : 'yhat_Bajas_Mes_CT', 'yhat_lower_' + column  : 'yhat_lower_Bajas_Mes_CT', 'yhat_upper_' + column : 'yhat_upper_Bajas_Mes_CT'}, inplace=True)\n",
    "    if column == 'Altas_Mes_C+T':\n",
    "            df_predictions.rename(columns = {column: 'Altas_Mes_CT', 'yhat_' + column  : 'yhat_Altas_Mes_CT', 'yhat_lower_' + column  : 'yhat_lower_Altas_Mes_CT', 'yhat_upper_' + column : 'yhat_upper_Altas_Mes_CT'}, inplace=True)\n",
    "  \n",
    "\n",
    "    # Guardamos las predicciones en un archivo .csv\n",
    "    df_predictions.to_csv('dataframes/full_data_predictions_' + column + '.csv', encoding = 'utf-8', sep=';', decimal=',', index = False)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se ha comentado en el apartado anterior, debido a las limitaciones del kernel de jupyter se almacenan los archivos \".csv\" en local. Una vez almacenados esta parte del codigo consta en cargar dichos archivos y unirlos para crear un dataset final con todas las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions_altas_m = pd.read_csv('../dataframes/full_data_predictions_Altas M.csv', encoding = 'utf-8', sep = ';')\n",
    "df_predictions_altas_m = pd.read_csv('../dataframes/full_data_predictions_Altas M.csv', encoding = 'utf-8', sep = ';')\n",
    "df_predictions_bajas_m = pd.read_csv('../dataframes/full_data_predictions_Bajas M.csv', encoding = 'utf-8', sep = ';')\n",
    "df_predictions_altas_mes_ct = pd.read_csv('../dataframes/full_data_predictions_Altas_Mes_C+T.csv', encoding = 'utf-8', sep = ';')\n",
    "df_predictions_bajas_mes_ct = pd.read_csv('../dataframes/full_data_predictions_Bajas_Mes_C+T.csv', encoding = 'utf-8', sep = ';')\n",
    "df_predictions_altas_mes_tra = pd.read_csv('../dataframes/full_data_predictions_Altas_Mes_Tra.csv', encoding = 'utf-8', sep = ';')\n",
    "df_predictions_bajas_mes_tra = pd.read_csv('../dataframes/full_data_predictions_Bajas_Mes_Tra.csv', encoding = 'utf-8', sep = ';')\n",
    "\n",
    "\n",
    "\n",
    "# Uniendo todos los dataframes en uno solo\n",
    "df_full_predictions = pd.merge(df_predictions_altas_m, \n",
    "                        df_predictions_bajas_m[['Bajas M', 'yhat_Bajas M', 'yhat_lower_Bajas M', 'yhat_upper_Bajas M']],\n",
    "                        how= 'left',\n",
    "                        left_index = True,\n",
    "                        right_index= True)\n",
    "\n",
    "df_full_predictions = pd.merge(df_full_predictions, \n",
    "                        df_predictions_altas_mes_ct[['Altas_Mes_CT', 'yhat_Altas_Mes_CT', 'yhat_lower_Altas_Mes_CT', 'yhat_upper_Altas_Mes_CT']],\n",
    "                        how= 'left',\n",
    "                        left_index = True,\n",
    "                        right_index= True)\n",
    "\n",
    "df_full_predictions = pd.merge(df_full_predictions, \n",
    "                        df_predictions_bajas_mes_ct[['Bajas_Mes_CT', 'yhat_Bajas_Mes_CT', 'yhat_lower_Bajas_Mes_CT', 'yhat_upper_Bajas_Mes_CT']],\n",
    "                        how= 'left',\n",
    "                        left_index = True,\n",
    "                        right_index= True)\n",
    "\n",
    "df_full_predictions = pd.merge(df_full_predictions, \n",
    "                        df_predictions_altas_mes_tra[['Altas_Mes_Tra', 'yhat_Altas_Mes_Tra', 'yhat_lower_Altas_Mes_Tra', 'yhat_upper_Altas_Mes_Tra']],\n",
    "                        how= 'left',\n",
    "                        left_index = True,\n",
    "                        right_index= True)\n",
    "\n",
    "df_full_predictions = pd.merge(df_full_predictions, \n",
    "                        df_predictions_bajas_mes_tra[['Bajas_Mes_Tra', 'yhat_Bajas_Mes_Tra', 'yhat_lower_Bajas_Mes_Tra', 'yhat_upper_Bajas_Mes_Tra']],\n",
    "                        how= 'left',\n",
    "                        left_index = True,\n",
    "                        right_index= True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correccion de predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al realizar las predicciones, algunos valores de altas o de bajas quedan como negativos o positivos respectivamente. Dado que esto no tiene sentido para el modelo de negocio el siguiente codigo corrige todos los valores inferiores a 0 en las altas y los superiores a 0 en las bajas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si hay columnas con el valor de altas en negativo, son transformadas a 0 que es el límite que tiene sentido\n",
    "for column in df_full_predictions.columns:\n",
    "    if 'Altas' in column:\n",
    "        temp_np_array = np.array(df_full_predictions[column].values.tolist())\n",
    "        df_full_predictions[column] = np.where(temp_np_array < 2, 0, temp_np_array).tolist()\n",
    "\n",
    "# Si hay columnas con el valor de bajas en positivo, son transformadas a 0 que es el límite que tiene sentido\n",
    "for column in df_full_predictions.columns:\n",
    "    if 'Bajas' in column:\n",
    "        temp_np_array = np.array(df_full_predictions[column].values.tolist())\n",
    "        df_full_predictions[column] = np.where(temp_np_array > -2, 0, temp_np_array).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardando el dataframe final\n",
    "df_full_predictions.to_csv('../dataframes/full_data_predictions.csv', encoding = 'utf-8', sep=';', decimal=',', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b26eca43ff2794ebfcd64423be45fc6ce9985487a083111d5dc1f18719dec52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
